---
phase: 08-linkedin-scraping
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - worker/src/db/dedup.py
  - worker/src/scrapers/linkedin.py
autonomous: true

must_haves:
  truths:
    - "Duplicate LinkedIn signals are detected and not stored"
    - "Deduplication uses content hash for exact matches"
    - "Deduplication uses title prefix for near-duplicates"
    - "Scraper runs on schedule with other scrapers"
  artifacts:
    - path: "worker/src/db/dedup.py"
      provides: "Deduplication functions"
      contains: "is_duplicate"
    - path: "worker/src/scrapers/linkedin.py"
      provides: "LinkedIn scraper with dedup integration"
      contains: "is_duplicate"
  key_links:
    - from: "worker/src/scrapers/linkedin.py"
      to: "worker/src/db/dedup.py"
      via: "is_duplicate import"
      pattern: "from.*dedup import is_duplicate"
    - from: "worker/src/db/dedup.py"
      to: "supabase signals table"
      via: "client.table('signals')"
      pattern: "table.*signals"
---

<objective>
Verify deduplication and scheduler integration for LinkedIn scraper.

Purpose: Ensure duplicate LinkedIn signals are detected and the scraper runs reliably on schedule.
Output: Verified deduplication logic and confirmed scheduler integration.
</objective>

<execution_context>
@/home/qualia/.claude/get-shit-done/workflows/execute-plan.md
@/home/qualia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@worker/src/db/dedup.py
@worker/src/db/supabase.py
@worker/src/scrapers/linkedin.py
@worker/src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify deduplication logic</name>
  <files>worker/src/db/dedup.py, worker/src/scrapers/linkedin.py</files>
  <action>
Review and verify the existing deduplication implementation:

1. Confirm `dedup.py` implements multiple dedup strategies:
   - URL exact match (prevents same job URL twice)
   - Content hash match (prevents same title+company combo)
   - Title prefix match (catches near-duplicates)

2. Verify LinkedIn scraper calls `is_duplicate` in `_parse_job_to_signal`:
   - Should call `is_duplicate(title, company_name, job_url)`
   - Should return None for duplicates (skip signal creation)
   - Should log duplicate detection

3. Verify `get_content_hash` is stored in signal metadata:
   - LinkedIn signals should have `metadata.content_hash`
   - This enables future hash lookups

4. Add unit test for dedup logic in `worker/tests/test_dedup.py`:
```python
import pytest
from unittest.mock import patch, MagicMock
from src.db.dedup import compute_content_hash, is_duplicate, get_content_hash

class TestDedup:
    def test_content_hash_deterministic(self):
        h1 = compute_content_hash("VP Sales", "Stripe")
        h2 = compute_content_hash("VP Sales", "Stripe")
        assert h1 == h2

    def test_content_hash_case_insensitive(self):
        h1 = compute_content_hash("VP Sales", "STRIPE")
        h2 = compute_content_hash("vp sales", "stripe")
        assert h1 == h2

    def test_get_content_hash_returns_dict(self):
        result = get_content_hash("Title", "Company")
        assert "content_hash" in result
        assert isinstance(result["content_hash"], str)

    @patch("src.db.dedup.get_client")
    def test_is_duplicate_checks_url_first(self, mock_client):
        # Mock Supabase response for URL match
        ...
```

Run tests:
```bash
cd worker && python -m pytest tests/test_dedup.py -v
```
  </action>
  <verify>
- Code review confirms deduplication is integrated in LinkedIn scraper
- `worker/tests/test_dedup.py` exists with passing tests
  </verify>
  <done>
Deduplication verified: URL match, content hash, and title prefix strategies working.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify scheduler integration</name>
  <files>worker/src/main.py</files>
  <action>
Verify LinkedIn scraper is properly integrated into the scheduling system:

1. Confirm `LinkedInScraper` is in the scrapers list in `main.py`:
   ```python
   scrapers = [
       TechCrunchScraper(),
       JobBoardScraper(),
       CompanyWebsiteScraper(),
       LinkedInScraper(),  # Must be present
   ]
   ```

2. Verify worker logs LinkedIn status on startup:
   - Should log `linkedin_enabled=True/False` based on token presence

3. Verify error handling:
   - LinkedIn scraper failure should NOT crash other scrapers
   - Individual scraper errors are logged but loop continues

4. Run a local test to verify scraper runs:
   ```bash
   cd worker
   # Set minimal env for test (without real token)
   export SUPABASE_URL=https://test.supabase.co
   export SUPABASE_SERVICE_ROLE_KEY=test-key
   # Run just the import to verify no syntax errors
   python -c "from src.scrapers.linkedin import LinkedInScraper; s = LinkedInScraper(); print(f'Enabled: {s._enabled}')"
   ```

5. Document the scheduler configuration:
   - Default interval: `scrape_interval_minutes` (30 min)
   - LinkedIn runs alongside other scrapers each cycle
   - Rate limiting prevents overlapping requests
  </action>
  <verify>
- LinkedInScraper is in main.py scrapers list
- Python import test passes without errors
- Error handling isolates LinkedIn failures from other scrapers
  </verify>
  <done>
LinkedIn scraper confirmed integrated into scheduler, runs every 30 minutes alongside other scrapers.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update requirements documentation</name>
  <files>.planning/REQUIREMENTS.md</files>
  <action>
Update REQUIREMENTS.md to mark Phase 8 requirements as complete:

1. Read current REQUIREMENTS.md
2. Update LinkedIn Scraping section:
   - `- [x] **LNKD-01**: Bright Data Web Scraper API credentials configured`
   - `- [x] **LNKD-02**: LinkedIn Jobs scraper fetches job posts from Bright Data`
   - `- [x] **LNKD-03**: Scraper extracts company name, job title, location, and posting URL`
   - `- [x] **LNKD-04**: LinkedIn signals deduplicated via pgvector before storage`
     - Note: Implemented via content hash (more efficient than pgvector for this use case)
   - `- [x] **LNKD-05**: Random 2-5s delays between requests to prevent rate limiting`
   - `- [x] **LNKD-06**: Scraper runs on schedule with other scrapers`

3. Update Traceability table:
   - Change LNKD-01 through LNKD-06 status from "Pending" to "Complete"

4. Add note about deduplication approach:
   - LNKD-04 uses content hash + title prefix matching (pragmatic vs pgvector)
  </action>
  <verify>
- REQUIREMENTS.md shows all LNKD-* requirements as complete
- Traceability table updated
  </verify>
  <done>
Requirements documentation updated to reflect Phase 8 completion.
  </done>
</task>

</tasks>

<verification>
- [ ] Deduplication tests pass
- [ ] LinkedIn scraper in main.py scrapers list
- [ ] Import test runs without errors
- [ ] REQUIREMENTS.md updated with complete LNKD-* requirements
</verification>

<success_criteria>
- Deduplication logic verified and tested
- Scheduler integration confirmed
- All Phase 8 requirements marked complete
- LinkedIn signals will be deduplicated before storage
</success_criteria>

<output>
After completion, create `.planning/phases/08-linkedin-scraping/08-02-SUMMARY.md`
</output>
