---
phase: 08-linkedin-scraping
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - worker/src/scrapers/linkedin.py
  - worker/src/config.py
autonomous: false

user_setup:
  - service: brightdata
    why: "LinkedIn scraping via Web Scraper API"
    env_vars:
      - name: BRIGHT_DATA_API_TOKEN
        source: "Bright Data Dashboard -> Web Scraper API -> API Token"
    dashboard_config:
      - task: "Ensure LinkedIn Jobs dataset is enabled"
        location: "Bright Data Dashboard -> Web Scraper API -> Datasets"

must_haves:
  truths:
    - "LinkedIn scraper fetches job data from Bright Data API"
    - "Each signal includes company, title, location, and source URL"
    - "Scraper gracefully skips when credentials are missing"
    - "Rate limiting delays between company requests"
  artifacts:
    - path: "worker/src/scrapers/linkedin.py"
      provides: "LinkedInScraper class with Bright Data integration"
      contains: "class LinkedInScraper"
    - path: "worker/src/config.py"
      provides: "BRIGHT_DATA_API_TOKEN configuration"
      contains: "bright_data_api_token"
  key_links:
    - from: "worker/src/scrapers/linkedin.py"
      to: "https://api.brightdata.com/datasets/v3/trigger"
      via: "httpx POST request"
      pattern: "api.brightdata.com"
    - from: "worker/src/main.py"
      to: "worker/src/scrapers/linkedin.py"
      via: "LinkedInScraper import"
      pattern: "LinkedInScraper"
---

<objective>
Verify and enhance Bright Data LinkedIn Jobs scraper integration.

Purpose: Enable LinkedIn job signal ingestion via Bright Data's legal, production-grade API.
Output: Working LinkedIn scraper that fetches real job data when credentials are configured.
</objective>

<execution_context>
@/home/qualia/.claude/get-shit-done/workflows/execute-plan.md
@/home/qualia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@worker/src/scrapers/linkedin.py
@worker/src/scrapers/base.py
@worker/src/config.py
@worker/src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify LinkedIn scraper implementation</name>
  <files>worker/src/scrapers/linkedin.py, worker/src/config.py</files>
  <action>
Review and verify the existing LinkedIn scraper implementation:

1. Confirm `linkedin.py` correctly implements Bright Data Web Scraper API:
   - Endpoint: `https://api.brightdata.com/datasets/v3/trigger`
   - Dataset ID: `gd_l1viktl72bvl7bjuj0` (LinkedIn Jobs)
   - Authorization header: Bearer token
   - Async polling for results via snapshot_id

2. Verify signal parsing extracts required fields:
   - company_name (from job data or fallback)
   - title (job title)
   - location (job location)
   - source_url (LinkedIn job URL)

3. Verify rate limiting:
   - Random 2-5s delay between company requests
   - Exponential backoff on retries (via tenacity)

4. Verify graceful degradation:
   - Returns empty list if BRIGHT_DATA_API_TOKEN not set
   - Logs warning without crashing

5. Run type check to ensure no typing issues:
   ```bash
   cd worker && python -m mypy src/scrapers/linkedin.py --ignore-missing-imports
   ```

6. If any issues found, fix them. Otherwise, confirm implementation is correct.
  </action>
  <verify>
- `python -m mypy src/scrapers/linkedin.py --ignore-missing-imports` passes
- Code review confirms all LNKD requirements are met
  </verify>
  <done>
LinkedIn scraper implementation verified correct with all required fields extracted.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add scraper unit test</name>
  <files>worker/tests/test_linkedin.py</files>
  <action>
Create a unit test for the LinkedIn scraper that validates parsing logic without making real API calls:

1. Create `worker/tests/test_linkedin.py`
2. Test `_parse_job_to_signal` method with mock job data
3. Test `_assess_priority` method for VP/Director detection
4. Test graceful skip when token not configured

Test structure:
```python
import pytest
from unittest.mock import patch, MagicMock
from src.scrapers.linkedin import LinkedInScraper, SIGNAL_KEYWORDS

class TestLinkedInScraper:
    def test_parse_job_extracts_required_fields(self):
        # Mock job data from Bright Data
        # Assert signal has company_name, title, location, source_url

    def test_parse_job_filters_non_signal_titles(self):
        # Job title without keywords -> returns None

    def test_assess_priority_high_for_vp(self):
        # VP/Director titles -> "high"

    def test_assess_priority_medium_for_others(self):
        # Regular titles -> "medium"

    def test_scraper_disabled_without_token(self):
        # No BRIGHT_DATA_API_TOKEN -> scraper._enabled is False
```

Run tests:
```bash
cd worker && python -m pytest tests/test_linkedin.py -v
```
  </action>
  <verify>
- `worker/tests/test_linkedin.py` exists
- `cd worker && python -m pytest tests/test_linkedin.py -v` passes
  </verify>
  <done>
LinkedIn scraper has unit tests validating parsing and priority logic.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
LinkedIn scraper verified with Bright Data API integration:
- Fetches job data from Bright Data Web Scraper API
- Extracts company, title, location, URL
- Rate limits with 2-5s random delays
- Unit tests validate parsing logic
  </what-built>
  <how-to-verify>
1. Check that BRIGHT_DATA_API_TOKEN is configured in Railway:
   - Visit Railway dashboard for the worker service
   - Variables tab -> verify BRIGHT_DATA_API_TOKEN exists

2. If token IS configured:
   - Trigger a manual worker run or wait for next scheduled run
   - Check Railway logs for `linkedin_scraper` entries
   - Verify signals appear in Supabase signals table with source_name="LinkedIn"

3. If token NOT configured:
   - Add BRIGHT_DATA_API_TOKEN to Railway environment variables
   - Source: Bright Data Dashboard -> Account -> API Tokens
   - Redeploy worker
   - Wait for scrape cycle, check logs

Expected log entries:
- `linkedin_scraper_skipped` (if no token) OR
- `scrape_complete scraper=linkedin signal_count=N` (if working)
  </how-to-verify>
  <resume-signal>Type "verified" when LinkedIn scraper is confirmed working, or describe issues</resume-signal>
</task>

</tasks>

<verification>
- [ ] LinkedIn scraper implementation passes type check
- [ ] Unit tests pass for parsing and priority logic
- [ ] Scraper integrated into main.py scrapers list
- [ ] BRIGHT_DATA_API_TOKEN documented in user_setup
</verification>

<success_criteria>
- LinkedIn scraper verified and tested
- User has clear path to configure Bright Data credentials
- Scraper ready to fetch real LinkedIn job data
</success_criteria>

<output>
After completion, create `.planning/phases/08-linkedin-scraping/08-01-SUMMARY.md`
</output>
