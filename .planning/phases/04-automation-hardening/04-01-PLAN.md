---
phase: 04-automation-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - worker/src/scrapers/linkedin.py
  - worker/src/main.py
  - worker/src/config.py
  - worker/pyproject.toml
autonomous: true

user_setup:
  - service: bright-data
    why: "LinkedIn scraping with legal protection"
    env_vars:
      - name: BRIGHT_DATA_API_TOKEN
        source: "Bright Data Dashboard -> Account -> API Token"
    dashboard_config:
      - task: "Create Scraping Browser zone for LinkedIn"
        location: "Bright Data Dashboard -> Proxies & Scraping -> Web Scraper API -> LinkedIn"

must_haves:
  truths:
    - "LinkedIn Jobs scraper runs on schedule with existing scrapers"
    - "Scraper uses Bright Data residential proxies for anti-detection"
    - "LinkedIn signals are stored with same schema as other signals"
    - "Rate limiting prevents bans (2-5s delays between requests)"
  artifacts:
    - path: "worker/src/scrapers/linkedin.py"
      provides: "LinkedIn Jobs scraper extending BaseScraper"
      min_lines: 60
    - path: "worker/src/main.py"
      provides: "LinkedIn scraper added to job loop"
      contains: "LinkedInScraper"
  key_links:
    - from: "worker/src/scrapers/linkedin.py"
      to: "worker/src/scrapers/base.py"
      via: "class inheritance"
      pattern: "class LinkedInScraper\\(BaseScraper\\)"
    - from: "worker/src/main.py"
      to: "worker/src/scrapers/linkedin.py"
      via: "import and instantiation"
      pattern: "from .scrapers.linkedin import LinkedInScraper"
---

<objective>
Add LinkedIn Jobs scraper to the worker using Bright Data SDK for legal compliance and anti-detection.

Purpose: Complete the signal ingestion pipeline by adding LinkedIn as the final data source. LinkedIn is the most valuable signal source for hiring intent but requires proxy rotation to avoid rate limiting and legal protection via Bright Data.

Output: LinkedIn scraper extending BaseScraper, integrated into main.py job loop, with configurable target companies and rate limiting.
</objective>

<execution_context>
@/home/qualia/.claude/get-shit-done/workflows/execute-plan.md
@/home/qualia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-automation-hardening/04-RESEARCH.md

# Worker infrastructure from Phase 2
@worker/src/scrapers/base.py
@worker/src/config.py
@worker/src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LinkedIn Jobs scraper with Bright Data</name>
  <files>
    worker/src/scrapers/linkedin.py
    worker/pyproject.toml
    worker/src/config.py
  </files>
  <action>
1. Add `brightdata-sdk` and `tenacity` to pyproject.toml dependencies

2. Update config.py to add:
   - `bright_data_api_token: Optional[str] = None` for SDK authentication
   - Keep existing proxy_url property for fallback

3. Create worker/src/scrapers/linkedin.py:
   - Class `LinkedInScraper(BaseScraper)` with `name = "linkedin"`
   - Implement `async scrape() -> list[Signal]` method
   - Use Bright Data Web Scraper API for LinkedIn Jobs (see 04-RESEARCH.md Pattern 1)
   - Hardcode initial target companies: ["Stripe", "Shopify", "Vercel", "Supabase", "Linear"]
   - Parse job listings into Signal objects with:
     - signal_type = "hiring"
     - priority = "medium" (AI enrichment will adjust)
     - source_name = "linkedin"
     - source_url = job listing URL
     - company_name from parsed data
     - title = job title
     - summary = job description snippet
   - Add random delays between company requests (2-5 seconds) using asyncio.sleep
   - Wrap requests in tenacity retry decorator (3 attempts, exponential backoff)
   - Log clearly when Bright Data credentials are missing (skip gracefully, don't crash)

Important: Do NOT use logged-in sessions or private data. Only scrape publicly visible job listings.
  </action>
  <verify>
    cd worker && python -c "from src.scrapers.linkedin import LinkedInScraper; print('LinkedIn scraper imports successfully')"
  </verify>
  <done>LinkedIn scraper class exists, extends BaseScraper, can be imported without errors</done>
</task>

<task type="auto">
  <name>Task 2: Integrate LinkedIn scraper into job loop</name>
  <files>
    worker/src/main.py
  </files>
  <action>
1. Import LinkedInScraper:
   `from .scrapers.linkedin import LinkedInScraper`

2. Add LinkedInScraper to scrapers list in run_scrapers():
   ```python
   scrapers = [
       TechCrunchScraper(),
       JobBoardScraper(),
       CompanyWebsiteScraper(),
       LinkedInScraper(),  # Add this
   ]
   ```

3. Add conditional logging for LinkedIn:
   - Log when LinkedIn scraper is skipped (no Bright Data credentials)
   - Log success count separately in scrape_cycle_complete

4. Update worker_starting log to include linkedin_enabled status
  </action>
  <verify>
    cd worker && python -c "from src.main import run_scrapers; print('Main imports with LinkedIn scraper')"
  </verify>
  <done>LinkedIn scraper integrated into job loop, conditional skip when no credentials</done>
</task>

</tasks>

<verification>
1. All imports work: `cd worker && python -c "from src.main import main"`
2. LinkedInScraper follows BaseScraper pattern
3. Rate limiting configured (2-5s delays)
4. Graceful skip when BRIGHT_DATA_API_TOKEN not set
5. Type check passes: `cd worker && python -m mypy src/ --ignore-missing-imports` (if mypy installed)
</verification>

<success_criteria>
- LinkedIn scraper class exists at worker/src/scrapers/linkedin.py
- Scraper is included in main.py job loop
- Scraper skips gracefully when Bright Data credentials missing
- Random delays between requests prevent rate limiting
- No runtime errors on import
</success_criteria>

<output>
After completion, create `.planning/phases/04-automation-hardening/04-01-SUMMARY.md`
</output>
