---
phase: 02-signal-ingestion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - worker/pyproject.toml
  - worker/src/main.py
  - worker/src/config.py
  - worker/src/db/supabase.py
  - worker/src/scrapers/base.py
  - worker/src/scrapers/news.py
  - worker/Dockerfile
  - worker/railway.toml
  - worker/.env.example
autonomous: true

user_setup:
  - service: railway
    why: "Host Python worker for long-running scraper jobs"
    env_vars:
      - name: RAILWAY_TOKEN
        source: "Railway Dashboard -> Settings -> Tokens -> Create new token"
    dashboard_config:
      - task: "Create new project"
        location: "Railway Dashboard -> New Project -> Empty Project"
  - service: supabase
    why: "Database access from Python worker"
    env_vars:
      - name: SUPABASE_URL
        source: "Supabase Dashboard -> Settings -> API -> Project URL"
      - name: SUPABASE_SERVICE_ROLE_KEY
        source: "Supabase Dashboard -> Settings -> API -> service_role key (secret)"

must_haves:
  truths:
    - "Python worker runs on Railway without errors"
    - "Worker can connect to Supabase and read/write signals table"
    - "News scraper fetches TechCrunch articles and extracts signal data"
    - "Scraped signals are stored in Supabase database"
  artifacts:
    - path: "worker/pyproject.toml"
      provides: "Python project config with dependencies"
      contains: "httpx"
    - path: "worker/src/main.py"
      provides: "Worker entry point with scheduler"
      contains: "schedule"
    - path: "worker/src/db/supabase.py"
      provides: "Supabase client wrapper"
      contains: "insert_signal"
    - path: "worker/src/scrapers/news.py"
      provides: "TechCrunch scraper"
      contains: "TechCrunchScraper"
  key_links:
    - from: "worker/src/main.py"
      to: "worker/src/scrapers/news.py"
      via: "import and schedule"
      pattern: "from scrapers.news import"
    - from: "worker/src/scrapers/news.py"
      to: "worker/src/db/supabase.py"
      via: "save scraped signals"
      pattern: "supabase.*insert_signal"
---

<objective>
Create Python worker project with TechCrunch news scraper, deployable to Railway.

Purpose: Establish the signal ingestion infrastructure that will power all scrapers. News sites are the first source because they have the most structured data (RSS feeds, consistent HTML).

Output: A new `worker/` directory containing a complete Python project ready for Railway deployment, with working TechCrunch scraper saving signals to Supabase.
</objective>

<execution_context>
@/home/qualia/.claude/get-shit-done/workflows/execute-plan.md
@/home/qualia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-01-SUMMARY.md

Key context from Phase 1:
- signals table schema: id, user_id, company_name, company_domain, signal_type, title, summary, source_url, source_name, priority, status, metadata
- signal_type values: hiring, funding, expansion, partnership, product_launch, leadership_change
- priority values: high, medium, low
- Supabase project URL and keys in .env
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Python project structure with dependencies</name>
  <files>
    worker/pyproject.toml
    worker/src/__init__.py
    worker/src/config.py
    worker/.env.example
    worker/.gitignore
  </files>
  <action>
Create a new `worker/` directory at repository root (sibling to `src/`).

**pyproject.toml** - Use modern Python packaging:
```toml
[project]
name = "axidex-worker"
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "httpx>=0.27",
    "selectolax>=0.3",
    "supabase>=2.0",
    "schedule>=1.2",
    "python-dotenv>=1.0",
    "pydantic>=2.0",
    "structlog>=24.0",
]

[project.optional-dependencies]
dev = ["pytest", "ruff"]
```

**src/config.py** - Pydantic settings for env vars:
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    supabase_url: str
    supabase_service_role_key: str
    scrape_interval_minutes: int = 30
    log_level: str = "INFO"

    class Config:
        env_file = ".env"

settings = Settings()
```

**.env.example** - Document required vars:
```
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbG...
SCRAPE_INTERVAL_MINUTES=30
LOG_LEVEL=INFO
```

**.gitignore** - Python ignores:
```
__pycache__/
*.pyc
.env
.venv/
dist/
*.egg-info/
```
  </action>
  <verify>
    cd worker && python -c "import sys; print(sys.version)" # Python 3.11+
    ls worker/pyproject.toml worker/src/config.py worker/.env.example
  </verify>
  <done>
    - worker/pyproject.toml exists with all dependencies
    - worker/src/config.py has Settings class with all env vars
    - worker/.env.example documents all required environment variables
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Supabase client wrapper and base scraper</name>
  <files>
    worker/src/db/__init__.py
    worker/src/db/supabase.py
    worker/src/scrapers/__init__.py
    worker/src/scrapers/base.py
    worker/src/models.py
  </files>
  <action>
**src/models.py** - Pydantic models matching signals table:
```python
from pydantic import BaseModel
from typing import Literal, Any
from datetime import datetime

SignalType = Literal['hiring', 'funding', 'expansion', 'partnership', 'product_launch', 'leadership_change']
Priority = Literal['high', 'medium', 'low']

class Signal(BaseModel):
    company_name: str
    company_domain: str | None = None
    signal_type: SignalType
    title: str
    summary: str
    source_url: str
    source_name: str
    priority: Priority = 'medium'
    metadata: dict[str, Any] = {}
```

**src/db/supabase.py** - Supabase operations:
```python
from supabase import create_client, Client
from ..config import settings
from ..models import Signal
import structlog

log = structlog.get_logger()

_client: Client | None = None

def get_client() -> Client:
    global _client
    if _client is None:
        _client = create_client(
            settings.supabase_url,
            settings.supabase_service_role_key
        )
    return _client

def insert_signal(signal: Signal, user_id: str) -> dict | None:
    """Insert a signal for a specific user. Returns inserted row or None on error."""
    try:
        client = get_client()
        data = signal.model_dump()
        data['user_id'] = user_id
        result = client.table('signals').insert(data).execute()
        log.info("signal_inserted", company=signal.company_name, type=signal.signal_type)
        return result.data[0] if result.data else None
    except Exception as e:
        log.error("signal_insert_failed", error=str(e), company=signal.company_name)
        return None

def signal_exists(source_url: str) -> bool:
    """Check if a signal with this source URL already exists (basic dedup)."""
    client = get_client()
    result = client.table('signals').select('id').eq('source_url', source_url).limit(1).execute()
    return len(result.data) > 0
```

**src/scrapers/base.py** - Abstract base scraper:
```python
from abc import ABC, abstractmethod
from ..models import Signal
import structlog

log = structlog.get_logger()

class BaseScraper(ABC):
    name: str = "base"

    @abstractmethod
    async def scrape(self) -> list[Signal]:
        """Scrape source and return list of signals."""
        pass

    def log_result(self, signals: list[Signal]):
        log.info(
            "scrape_complete",
            scraper=self.name,
            signal_count=len(signals)
        )
```
  </action>
  <verify>
    cd worker && python -c "from src.models import Signal; print(Signal.__fields__.keys())"
  </verify>
  <done>
    - Signal model matches database schema
    - Supabase client can insert signals and check for duplicates
    - BaseScraper provides interface for all scrapers
  </done>
</task>

<task type="auto">
  <name>Task 3: Create TechCrunch news scraper and main entry point</name>
  <files>
    worker/src/scrapers/news.py
    worker/src/main.py
    worker/Dockerfile
    worker/railway.toml
  </files>
  <action>
**src/scrapers/news.py** - TechCrunch scraper using RSS + HTML parsing:
```python
import httpx
from selectolax.parser import HTMLParser
from ..scrapers.base import BaseScraper
from ..models import Signal
from ..db.supabase import signal_exists
import structlog
import re

log = structlog.get_logger()

# TechCrunch funding/startup RSS feeds
TC_FEEDS = [
    "https://techcrunch.com/category/startups/feed/",
    "https://techcrunch.com/category/venture/feed/",
]

class TechCrunchScraper(BaseScraper):
    name = "techcrunch"

    async def scrape(self) -> list[Signal]:
        signals = []
        async with httpx.AsyncClient(timeout=30.0) as client:
            for feed_url in TC_FEEDS:
                try:
                    resp = await client.get(feed_url)
                    resp.raise_for_status()
                    signals.extend(self._parse_feed(resp.text))
                except Exception as e:
                    log.error("feed_fetch_failed", feed=feed_url, error=str(e))

        # Filter already-seen signals
        new_signals = [s for s in signals if not signal_exists(s.source_url)]
        self.log_result(new_signals)
        return new_signals

    def _parse_feed(self, xml: str) -> list[Signal]:
        """Parse RSS XML into Signal objects."""
        signals = []
        parser = HTMLParser(xml)

        for item in parser.css("item"):
            try:
                title_el = item.css_first("title")
                link_el = item.css_first("link")
                desc_el = item.css_first("description")

                if not all([title_el, link_el, desc_el]):
                    continue

                title = title_el.text(strip=True)
                link = link_el.text(strip=True)
                description = desc_el.text(strip=True)

                # Extract company name from title (often "Company raises $X" or "Company launches Y")
                company = self._extract_company(title)
                signal_type = self._classify_signal(title, description)
                priority = self._assess_priority(title, description)

                if company and signal_type:
                    signals.append(Signal(
                        company_name=company,
                        signal_type=signal_type,
                        title=title,
                        summary=description[:500],  # Truncate
                        source_url=link,
                        source_name="TechCrunch",
                        priority=priority,
                        metadata={"raw_title": title}
                    ))
            except Exception as e:
                log.warning("item_parse_failed", error=str(e))

        return signals

    def _extract_company(self, title: str) -> str | None:
        """Basic company extraction - first capitalized word(s)."""
        # Pattern: "Company raises/launches/announces..."
        match = re.match(r'^([A-Z][a-zA-Z0-9]+(?:\s+[A-Z][a-zA-Z0-9]+)?)', title)
        return match.group(1) if match else None

    def _classify_signal(self, title: str, description: str) -> str | None:
        """Rule-based classification."""
        text = f"{title} {description}".lower()

        if any(kw in text for kw in ['raises', 'funding', 'series', 'seed', 'valuation', 'investment']):
            return 'funding'
        if any(kw in text for kw in ['hiring', 'jobs', 'talent', 'team']):
            return 'hiring'
        if any(kw in text for kw in ['expands', 'expansion', 'opens', 'new market', 'international']):
            return 'expansion'
        if any(kw in text for kw in ['partnership', 'partners with', 'collaborat']):
            return 'partnership'
        if any(kw in text for kw in ['launches', 'announces', 'introduces', 'unveils', 'releases']):
            return 'product_launch'
        if any(kw in text for kw in ['ceo', 'cto', 'appoints', 'joins as', 'new head']):
            return 'leadership_change'

        return None  # Can't classify

    def _assess_priority(self, title: str, description: str) -> str:
        """Rule-based priority scoring."""
        text = f"{title} {description}".lower()

        # High priority: Large funding rounds, major launches
        if any(kw in text for kw in ['$100m', '$50m', 'unicorn', 'series c', 'series d', 'ipo']):
            return 'high'

        # Low priority: Small rounds, minor updates
        if any(kw in text for kw in ['seed', 'pre-seed', 'angel', 'update']):
            return 'low'

        return 'medium'
```

**src/main.py** - Entry point with scheduler:
```python
import asyncio
import schedule
import time
import structlog
from .config import settings
from .scrapers.news import TechCrunchScraper
from .db.supabase import insert_signal, get_client

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
log = structlog.get_logger()

# Demo user ID - in production, signals would be assigned to users based on their preferences
DEMO_USER_ID = "00000000-0000-0000-0000-000000000000"

async def run_scrapers():
    """Run all scrapers and store results."""
    log.info("scrape_cycle_start")

    scrapers = [
        TechCrunchScraper(),
    ]

    for scraper in scrapers:
        try:
            signals = await scraper.scrape()
            for signal in signals:
                insert_signal(signal, DEMO_USER_ID)
        except Exception as e:
            log.error("scraper_failed", scraper=scraper.name, error=str(e))

    log.info("scrape_cycle_complete")

def job():
    """Wrapper to run async scrapers from sync scheduler."""
    asyncio.run(run_scrapers())

def main():
    log.info("worker_starting", interval=settings.scrape_interval_minutes)

    # Run immediately on start
    job()

    # Then schedule regular runs
    schedule.every(settings.scrape_interval_minutes).minutes.do(job)

    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == "__main__":
    main()
```

**Dockerfile** - Python 3.11 container:
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY pyproject.toml ./
RUN pip install --no-cache-dir .

# Copy source
COPY src/ ./src/

# Run worker
CMD ["python", "-m", "src.main"]
```

**railway.toml** - Railway config:
```toml
[build]
builder = "dockerfile"
dockerfilePath = "./Dockerfile"

[deploy]
startCommand = "python -m src.main"
restartPolicyType = "always"
```
  </action>
  <verify>
    cd worker && python -c "from src.scrapers.news import TechCrunchScraper; print('TechCrunch scraper imported')"
    ls worker/Dockerfile worker/railway.toml
  </verify>
  <done>
    - TechCrunch scraper parses RSS feeds and extracts signals
    - Main entry point runs scrapers on schedule
    - Dockerfile ready for Railway deployment
    - railway.toml configures auto-restart
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Python project structure exists:
   ```bash
   ls worker/pyproject.toml worker/src/main.py worker/Dockerfile
   ```

2. Import chain works:
   ```bash
   cd worker && python -c "from src.scrapers.news import TechCrunchScraper; from src.db.supabase import insert_signal; print('All imports OK')"
   ```

3. Docker build succeeds (optional - verifies Dockerfile):
   ```bash
   cd worker && docker build -t axidex-worker . 2>&1 | tail -3
   ```
</verification>

<success_criteria>
- worker/ directory exists with complete Python project
- TechCrunch scraper can parse RSS feeds
- Supabase client wrapper handles signal insertion
- Dockerfile and railway.toml ready for deployment
- All Python imports work without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-signal-ingestion/02-01-SUMMARY.md`

Include:
- Files created (worker/ structure)
- Key decisions (Python version, scheduler choice, RSS parsing approach)
- User setup instructions for Railway deployment
- Dependencies satisfied for next plans
</output>
