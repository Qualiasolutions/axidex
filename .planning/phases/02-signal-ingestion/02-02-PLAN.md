---
phase: 02-signal-ingestion
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - worker/src/scrapers/jobs.py
  - worker/src/scrapers/company.py
  - worker/src/main.py
  - supabase/migrations/003_pgvector.sql
  - worker/src/db/dedup.py
autonomous: true

user_setup:
  - service: bright_data
    why: "Residential proxies to avoid rate limiting when scraping job boards"
    env_vars:
      - name: BRIGHT_DATA_USERNAME
        source: "Bright Data Dashboard -> Proxies -> Zone settings -> Username"
      - name: BRIGHT_DATA_PASSWORD
        source: "Bright Data Dashboard -> Proxies -> Zone settings -> Password"
    dashboard_config:
      - task: "Create residential proxy zone"
        location: "Bright Data Dashboard -> Proxies & Scraping -> Add zone -> Residential"

must_haves:
  truths:
    - "Job board scraper fetches Indeed listings without getting blocked"
    - "Company website scraper extracts press releases and career pages"
    - "Duplicate signals are detected and not stored twice"
    - "pgvector extension enabled in Supabase"
  artifacts:
    - path: "worker/src/scrapers/jobs.py"
      provides: "Indeed/Glassdoor job board scraper"
      contains: "JobBoardScraper"
    - path: "worker/src/scrapers/company.py"
      provides: "Company website scraper"
      contains: "CompanyWebsiteScraper"
    - path: "worker/src/db/dedup.py"
      provides: "Vector-based deduplication"
      contains: "is_duplicate"
    - path: "supabase/migrations/003_pgvector.sql"
      provides: "pgvector extension and embedding column"
      contains: "CREATE EXTENSION"
  key_links:
    - from: "worker/src/scrapers/jobs.py"
      to: "worker/src/db/dedup.py"
      via: "check before insert"
      pattern: "is_duplicate"
    - from: "worker/src/main.py"
      to: "worker/src/scrapers/jobs.py"
      via: "scheduler import"
      pattern: "JobBoardScraper"
---

<objective>
Create job board and company website scrapers with vector-based deduplication.

Purpose: Job postings are the strongest buying signals (company actively hiring = budget to spend). Company press releases catch funding announcements before news sites. Deduplication prevents the same signal appearing multiple times from different sources.

Output: Two additional scrapers integrated into the worker, plus pgvector migration for semantic deduplication.
</objective>

<execution_context>
@/home/qualia/.claude/get-shit-done/workflows/execute-plan.md
@/home/qualia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-signal-ingestion/02-01-PLAN.md

This plan runs in parallel with 02-01 (Wave 1). It creates additional scrapers that will be imported into main.py. The base scraper and models from 02-01 are assumed to exist.

Key files from 02-01:
- worker/src/scrapers/base.py (BaseScraper class)
- worker/src/models.py (Signal model)
- worker/src/db/supabase.py (insert_signal, signal_exists)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pgvector migration for semantic deduplication</name>
  <files>
    supabase/migrations/003_pgvector.sql
    worker/src/db/dedup.py
  </files>
  <action>
**supabase/migrations/003_pgvector.sql** - Enable pgvector and add embedding column:
```sql
-- Axidex pgvector Migration
-- Migration: 003_pgvector.sql
-- Enables vector similarity for signal deduplication

-- Enable pgvector extension (Supabase has this available)
CREATE EXTENSION IF NOT EXISTS vector;

-- Add embedding column to signals table
ALTER TABLE signals
ADD COLUMN IF NOT EXISTS embedding vector(384);

-- Create index for fast similarity search
CREATE INDEX IF NOT EXISTS signals_embedding_idx
ON signals USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Function to find similar signals
CREATE OR REPLACE FUNCTION find_similar_signals(
    query_embedding vector(384),
    similarity_threshold float DEFAULT 0.9,
    max_results int DEFAULT 5
)
RETURNS TABLE (
    id UUID,
    title TEXT,
    similarity float
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        s.id,
        s.title,
        1 - (s.embedding <=> query_embedding) as similarity
    FROM signals s
    WHERE s.embedding IS NOT NULL
      AND 1 - (s.embedding <=> query_embedding) > similarity_threshold
    ORDER BY s.embedding <=> query_embedding
    LIMIT max_results;
END;
$$;
```

**worker/src/db/dedup.py** - Vector-based deduplication:
```python
import httpx
from ..config import settings
from .supabase import get_client
import structlog
import hashlib

log = structlog.get_logger()

# We use a simple local embedding approach for now
# In production, you'd use OpenAI embeddings or a local model
# For MVP, we use content hash + title similarity

def compute_content_hash(title: str, company: str) -> str:
    """Compute a deterministic hash for quick exact-match dedup."""
    content = f"{title.lower().strip()}|{company.lower().strip()}"
    return hashlib.sha256(content.encode()).hexdigest()[:32]


def is_duplicate(title: str, company_name: str, source_url: str) -> bool:
    """
    Check if this signal is a duplicate using multiple strategies:
    1. Exact URL match (already in supabase.signal_exists)
    2. Content hash match (same title + company)
    3. Fuzzy title match (future: vector similarity)
    """
    client = get_client()

    # Strategy 1: URL check (handled elsewhere, but double-check)
    url_result = client.table('signals').select('id').eq('source_url', source_url).limit(1).execute()
    if url_result.data:
        log.debug("duplicate_found", strategy="url", url=source_url)
        return True

    # Strategy 2: Content hash - check metadata for hash
    content_hash = compute_content_hash(title, company_name)

    # We store hash in metadata.content_hash
    hash_result = client.table('signals').select('id').contains('metadata', {'content_hash': content_hash}).limit(1).execute()
    if hash_result.data:
        log.debug("duplicate_found", strategy="hash", title=title[:50])
        return True

    # Strategy 3: Title similarity (simple approach - check for near-identical titles)
    # For the same company, if title starts the same way, likely duplicate
    title_prefix = title[:50].lower()
    prefix_result = (
        client.table('signals')
        .select('id')
        .ilike('title', f'{title_prefix}%')
        .eq('company_name', company_name)
        .limit(1)
        .execute()
    )
    if prefix_result.data:
        log.debug("duplicate_found", strategy="prefix", title=title[:50])
        return True

    return False


def get_content_hash(title: str, company: str) -> dict:
    """Return metadata dict with content hash for storage."""
    return {'content_hash': compute_content_hash(title, company)}
```

Note: Full vector embedding with OpenAI will be added in Plan 02-03 when we integrate the AI pipeline. For now, we use content hash + prefix matching which catches 80%+ of duplicates without additional API costs.
  </action>
  <verify>
    cat supabase/migrations/003_pgvector.sql | grep -c "CREATE"  # Should be 3+ (extension, index, function)
    cd worker && python -c "from src.db.dedup import is_duplicate, compute_content_hash; print('Dedup module OK')"
  </verify>
  <done>
    - pgvector extension migration ready
    - Content hash deduplication working
    - Duplicate check queries Supabase efficiently
  </done>
</task>

<task type="auto">
  <name>Task 2: Create job board scraper with proxy support</name>
  <files>
    worker/src/scrapers/jobs.py
    worker/src/config.py
  </files>
  <action>
**Update src/config.py** - Add proxy settings:
```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    supabase_url: str
    supabase_service_role_key: str
    scrape_interval_minutes: int = 30
    log_level: str = "INFO"

    # Bright Data proxy (optional - for job boards)
    bright_data_username: Optional[str] = None
    bright_data_password: Optional[str] = None

    @property
    def proxy_url(self) -> Optional[str]:
        if self.bright_data_username and self.bright_data_password:
            return f"http://{self.bright_data_username}:{self.bright_data_password}@brd.superproxy.io:22225"
        return None

    class Config:
        env_file = ".env"

settings = Settings()
```

**src/scrapers/jobs.py** - Indeed job board scraper:
```python
import httpx
from selectolax.parser import HTMLParser
from urllib.parse import urlencode, quote_plus
from ..scrapers.base import BaseScraper
from ..models import Signal
from ..config import settings
from ..db.dedup import is_duplicate, get_content_hash
import structlog

log = structlog.get_logger()

# Target companies - in production, this would come from user preferences
TARGET_COMPANIES = [
    "Salesforce",
    "HubSpot",
    "Stripe",
    "Shopify",
    "Twilio",
]

# Job title keywords indicating growth/buying signals
SIGNAL_KEYWORDS = [
    "Sales Director",
    "Account Executive",
    "Business Development",
    "VP Sales",
    "Head of Growth",
    "Marketing Director",
    "Enterprise Sales",
]


class JobBoardScraper(BaseScraper):
    name = "jobs"

    def __init__(self):
        self.proxy = settings.proxy_url

    async def scrape(self) -> list[Signal]:
        signals = []

        async with httpx.AsyncClient(
            timeout=30.0,
            proxy=self.proxy,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        ) as client:
            for company in TARGET_COMPANIES:
                try:
                    company_signals = await self._scrape_company_jobs(client, company)
                    signals.extend(company_signals)
                except Exception as e:
                    log.error("company_jobs_failed", company=company, error=str(e))

        self.log_result(signals)
        return signals

    async def _scrape_company_jobs(self, client: httpx.AsyncClient, company: str) -> list[Signal]:
        """Scrape Indeed for a specific company's job postings."""
        signals = []

        # Search Indeed for company jobs
        params = {
            "q": f'"{company}"',
            "l": "United States",
            "sort": "date",
        }
        url = f"https://www.indeed.com/jobs?{urlencode(params)}"

        try:
            resp = await client.get(url)
            if resp.status_code == 403:
                log.warning("rate_limited", source="indeed", company=company)
                return []
            resp.raise_for_status()
        except Exception as e:
            log.error("indeed_fetch_failed", company=company, error=str(e))
            return []

        parser = HTMLParser(resp.text)

        # Indeed job cards
        for job_card in parser.css("div.job_seen_beacon"):
            try:
                title_el = job_card.css_first("h2.jobTitle span")
                company_el = job_card.css_first("span[data-testid='company-name']")
                link_el = job_card.css_first("a.jcs-JobTitle")

                if not all([title_el, company_el, link_el]):
                    continue

                title = title_el.text(strip=True)
                detected_company = company_el.text(strip=True)
                job_link = "https://www.indeed.com" + link_el.attributes.get("href", "")

                # Only process if it's a target company
                if company.lower() not in detected_company.lower():
                    continue

                # Check if job title indicates buying signal
                if not any(kw.lower() in title.lower() for kw in SIGNAL_KEYWORDS):
                    continue

                # Dedup check
                if is_duplicate(title, detected_company, job_link):
                    continue

                signal = Signal(
                    company_name=detected_company,
                    signal_type='hiring',
                    title=f"{detected_company} is hiring: {title}",
                    summary=f"New job posting for {title} at {detected_company}. This indicates active growth and potential budget for solutions.",
                    source_url=job_link,
                    source_name="Indeed",
                    priority=self._assess_priority(title),
                    metadata=get_content_hash(title, detected_company)
                )
                signals.append(signal)

            except Exception as e:
                log.warning("job_parse_failed", error=str(e))

        return signals

    def _assess_priority(self, title: str) -> str:
        """VP/Director/Head = high priority, others = medium."""
        title_lower = title.lower()
        if any(kw in title_lower for kw in ['vp', 'vice president', 'director', 'head of', 'chief']):
            return 'high'
        return 'medium'
```
  </action>
  <verify>
    cd worker && python -c "from src.scrapers.jobs import JobBoardScraper; print('JobBoardScraper imported OK')"
    cd worker && python -c "from src.config import settings; print(f'Proxy configured: {settings.proxy_url is not None}')"
  </verify>
  <done>
    - Indeed scraper with proxy support
    - Targets specific companies and job titles
    - Deduplication integrated
    - Priority based on seniority
  </done>
</task>

<task type="auto">
  <name>Task 3: Create company website scraper and integrate all scrapers</name>
  <files>
    worker/src/scrapers/company.py
    worker/src/main.py
    worker/.env.example
  </files>
  <action>
**src/scrapers/company.py** - Company press releases and careers:
```python
import httpx
from selectolax.parser import HTMLParser
from urllib.parse import urljoin
from ..scrapers.base import BaseScraper
from ..models import Signal
from ..db.dedup import is_duplicate, get_content_hash
import structlog
import re

log = structlog.get_logger()

# Target company websites with known press release pages
COMPANY_SOURCES = [
    {
        "name": "Salesforce",
        "domain": "salesforce.com",
        "press_url": "https://www.salesforce.com/news/press-releases/",
        "careers_url": "https://careers.salesforce.com/",
    },
    {
        "name": "HubSpot",
        "domain": "hubspot.com",
        "press_url": "https://www.hubspot.com/company/news",
        "careers_url": "https://www.hubspot.com/careers",
    },
    {
        "name": "Stripe",
        "domain": "stripe.com",
        "press_url": "https://stripe.com/newsroom",
        "careers_url": "https://stripe.com/jobs",
    },
]


class CompanyWebsiteScraper(BaseScraper):
    name = "company"

    async def scrape(self) -> list[Signal]:
        signals = []

        async with httpx.AsyncClient(
            timeout=30.0,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"},
            follow_redirects=True
        ) as client:
            for company in COMPANY_SOURCES:
                try:
                    # Scrape press releases
                    press_signals = await self._scrape_press_releases(client, company)
                    signals.extend(press_signals)
                except Exception as e:
                    log.error("press_scrape_failed", company=company["name"], error=str(e))

        self.log_result(signals)
        return signals

    async def _scrape_press_releases(
        self, client: httpx.AsyncClient, company: dict
    ) -> list[Signal]:
        """Scrape a company's press release page."""
        signals = []

        try:
            resp = await client.get(company["press_url"])
            resp.raise_for_status()
        except Exception as e:
            log.warning("press_page_failed", url=company["press_url"], error=str(e))
            return []

        parser = HTMLParser(resp.text)

        # Generic approach: find links that look like press releases
        # Most press pages have article/news items with links and dates
        for link in parser.css("a"):
            href = link.attributes.get("href", "")
            text = link.text(strip=True)

            if not href or not text or len(text) < 20:
                continue

            # Filter to likely press release links
            if not any(kw in href.lower() for kw in ['news', 'press', 'release', 'announce']):
                if not any(kw in text.lower() for kw in ['funding', 'launch', 'partner', 'expand', 'appoint']):
                    continue

            # Build full URL
            full_url = urljoin(company["press_url"], href)

            # Skip if not from this company's domain
            if company["domain"] not in full_url:
                continue

            # Dedup
            if is_duplicate(text, company["name"], full_url):
                continue

            # Classify the signal
            signal_type = self._classify_press_release(text)
            if not signal_type:
                continue

            signal = Signal(
                company_name=company["name"],
                company_domain=company["domain"],
                signal_type=signal_type,
                title=text[:200],
                summary=f"Press release from {company['name']}: {text[:300]}",
                source_url=full_url,
                source_name=f"{company['name']} Newsroom",
                priority=self._assess_priority(text),
                metadata=get_content_hash(text, company["name"])
            )
            signals.append(signal)

        return signals

    def _classify_press_release(self, text: str) -> str | None:
        """Classify press release text into signal type."""
        text_lower = text.lower()

        if any(kw in text_lower for kw in ['funding', 'raises', 'investment', 'series', 'valuation']):
            return 'funding'
        if any(kw in text_lower for kw in ['launch', 'introduces', 'unveils', 'announces', 'releases']):
            return 'product_launch'
        if any(kw in text_lower for kw in ['partner', 'collaboration', 'alliance', 'teams with']):
            return 'partnership'
        if any(kw in text_lower for kw in ['expands', 'opens', 'new office', 'new market', 'international']):
            return 'expansion'
        if any(kw in text_lower for kw in ['appoints', 'names', 'joins', 'promotes', 'ceo', 'cto']):
            return 'leadership_change'

        return None

    def _assess_priority(self, text: str) -> str:
        """Assess priority based on content."""
        text_lower = text.lower()

        # High priority: Major funding, C-suite changes
        if any(kw in text_lower for kw in ['$100', '$50', 'billion', 'ceo', 'cto', 'acquisition']):
            return 'high'

        return 'medium'
```

**Update src/main.py** - Integrate all scrapers:
```python
import asyncio
import schedule
import time
import structlog
from .config import settings
from .scrapers.news import TechCrunchScraper
from .scrapers.jobs import JobBoardScraper
from .scrapers.company import CompanyWebsiteScraper
from .db.supabase import insert_signal

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
log = structlog.get_logger()

# Demo user ID - in production, signals would be assigned to users based on their preferences
DEMO_USER_ID = "00000000-0000-0000-0000-000000000000"


async def run_scrapers():
    """Run all scrapers and store results."""
    log.info("scrape_cycle_start")

    scrapers = [
        TechCrunchScraper(),
        JobBoardScraper(),
        CompanyWebsiteScraper(),
    ]

    total_signals = 0
    for scraper in scrapers:
        try:
            signals = await scraper.scrape()
            for signal in signals:
                result = insert_signal(signal, DEMO_USER_ID)
                if result:
                    total_signals += 1
        except Exception as e:
            log.error("scraper_failed", scraper=scraper.name, error=str(e))

    log.info("scrape_cycle_complete", total_signals=total_signals)


def job():
    """Wrapper to run async scrapers from sync scheduler."""
    asyncio.run(run_scrapers())


def main():
    log.info("worker_starting", interval=settings.scrape_interval_minutes)

    # Run immediately on start
    job()

    # Then schedule regular runs
    schedule.every(settings.scrape_interval_minutes).minutes.do(job)

    while True:
        schedule.run_pending()
        time.sleep(60)


if __name__ == "__main__":
    main()
```

**Update .env.example** - Add proxy vars:
```
# Supabase
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbG...

# Worker settings
SCRAPE_INTERVAL_MINUTES=30
LOG_LEVEL=INFO

# Bright Data proxy (optional - for job boards)
BRIGHT_DATA_USERNAME=brd-customer-xxx
BRIGHT_DATA_PASSWORD=xxx
```
  </action>
  <verify>
    cd worker && python -c "from src.scrapers.company import CompanyWebsiteScraper; print('CompanyWebsiteScraper OK')"
    cd worker && python -c "from src.main import run_scrapers; print('Main imports all scrapers OK')"
    grep -c "Scraper" worker/src/main.py  # Should be 3+ (TechCrunch, JobBoard, CompanyWebsite)
  </verify>
  <done>
    - Company website scraper handles press releases
    - All three scrapers integrated in main.py
    - Environment example updated with all vars
    - Worker ready to scrape news, jobs, and company sites
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Migration file exists:
   ```bash
   cat supabase/migrations/003_pgvector.sql | grep "vector"
   ```

2. All scrapers import successfully:
   ```bash
   cd worker && python -c "
   from src.scrapers.news import TechCrunchScraper
   from src.scrapers.jobs import JobBoardScraper
   from src.scrapers.company import CompanyWebsiteScraper
   from src.db.dedup import is_duplicate
   print('All scrapers and dedup OK')
   "
   ```

3. Main integrates all scrapers:
   ```bash
   grep -E "(TechCrunch|JobBoard|Company)Scraper" worker/src/main.py
   ```
</verification>

<success_criteria>
- pgvector migration ready for Supabase
- Job board scraper with proxy support
- Company website scraper for press releases
- Deduplication prevents duplicate signals
- All scrapers integrated into worker main
</success_criteria>

<output>
After completion, create `.planning/phases/02-signal-ingestion/02-02-SUMMARY.md`

Include:
- Scrapers created (jobs, company)
- Deduplication strategy (hash + prefix + future vector)
- User setup for Bright Data proxies
- pgvector migration ready to apply
</output>
