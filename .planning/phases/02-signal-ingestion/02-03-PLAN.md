---
phase: 02-signal-ingestion
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - worker/src/ai/__init__.py
  - worker/src/ai/extract.py
  - worker/src/ai/classify.py
  - worker/src/scrapers/base.py
  - worker/src/main.py
  - worker/pyproject.toml
autonomous: true

user_setup:
  - service: openai
    why: "GPT-4o for entity extraction and signal classification"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Platform -> API keys -> Create new secret key"
    dashboard_config:
      - task: "Set usage limits"
        location: "OpenAI Platform -> Settings -> Limits -> Set monthly budget"

must_haves:
  truths:
    - "Raw signal content is enriched with extracted entities (company, role, funding amount)"
    - "Each signal has an AI-determined type (hiring/funding/expansion/etc)"
    - "Each signal has an AI-determined priority score"
    - "AI processing happens after scraping, before database insert"
  artifacts:
    - path: "worker/src/ai/extract.py"
      provides: "Entity extraction using GPT-4o"
      contains: "extract_entities"
    - path: "worker/src/ai/classify.py"
      provides: "Signal classification and scoring"
      contains: "classify_signal"
    - path: "worker/src/scrapers/base.py"
      provides: "AI enrichment in base scraper"
      contains: "enrich_signal"
  key_links:
    - from: "worker/src/scrapers/base.py"
      to: "worker/src/ai/extract.py"
      via: "enrich before insert"
      pattern: "extract_entities"
    - from: "worker/src/ai/classify.py"
      to: "worker/src/ai/extract.py"
      via: "uses extracted entities"
      pattern: "import.*extract"
---

<objective>
Integrate OpenAI GPT-4o for entity extraction, signal classification, and priority scoring.

Purpose: Rule-based classification in 02-01/02-02 catches obvious cases, but AI handles nuanced signals (e.g., "Acme acquires TechCorp's AI division" = expansion + hiring signal). AI also extracts structured entities (funding amounts, role titles) that improve email generation quality in Phase 3.

Output: AI pipeline that processes raw scraped content, extracts entities, classifies signal type, and assigns priority scores. All integrated into the scraper flow before database insert.
</objective>

<execution_context>
@/home/qualia/.claude/get-shit-done/workflows/execute-plan.md
@/home/qualia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-signal-ingestion/02-01-PLAN.md
@.planning/phases/02-signal-ingestion/02-02-PLAN.md

Dependencies from prior plans:
- worker/src/models.py (Signal model with metadata field)
- worker/src/scrapers/base.py (BaseScraper class)
- worker/src/config.py (Settings with env vars)

Key schema fields that AI will populate:
- signal_type: hiring, funding, expansion, partnership, product_launch, leadership_change
- priority: high, medium, low
- metadata: JSON field for extracted entities (funding_amount, role_title, etc)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenAI client and entity extraction module</name>
  <files>
    worker/src/ai/__init__.py
    worker/src/ai/extract.py
    worker/src/config.py
    worker/pyproject.toml
  </files>
  <action>
**Update pyproject.toml** - Add OpenAI dependency:
```toml
[project]
name = "axidex-worker"
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "httpx>=0.27",
    "selectolax>=0.3",
    "supabase>=2.0",
    "schedule>=1.2",
    "python-dotenv>=1.0",
    "pydantic>=2.0",
    "pydantic-settings>=2.0",
    "structlog>=24.0",
    "openai>=1.0",
]

[project.optional-dependencies]
dev = ["pytest", "ruff"]
```

**Update src/config.py** - Add OpenAI settings:
```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # Supabase
    supabase_url: str
    supabase_service_role_key: str

    # Worker
    scrape_interval_minutes: int = 30
    log_level: str = "INFO"

    # Bright Data proxy (optional)
    bright_data_username: Optional[str] = None
    bright_data_password: Optional[str] = None

    # OpenAI
    openai_api_key: Optional[str] = None
    openai_model: str = "gpt-4o-mini"  # Use mini for cost efficiency, upgrade to gpt-4o for quality
    ai_enabled: bool = True  # Can disable AI for testing

    @property
    def proxy_url(self) -> Optional[str]:
        if self.bright_data_username and self.bright_data_password:
            return f"http://{self.bright_data_username}:{self.bright_data_password}@brd.superproxy.io:22225"
        return None

    class Config:
        env_file = ".env"

settings = Settings()
```

**src/ai/__init__.py**:
```python
from .extract import extract_entities
from .classify import classify_signal, score_priority

__all__ = ["extract_entities", "classify_signal", "score_priority"]
```

**src/ai/extract.py** - Entity extraction:
```python
from openai import OpenAI
from ..config import settings
import structlog
import json

log = structlog.get_logger()

_client: OpenAI | None = None

def get_openai_client() -> OpenAI:
    global _client
    if _client is None:
        if not settings.openai_api_key:
            raise ValueError("OPENAI_API_KEY not set")
        _client = OpenAI(api_key=settings.openai_api_key)
    return _client


EXTRACTION_PROMPT = """Extract structured entities from this business signal. Return JSON only.

Signal:
Title: {title}
Summary: {summary}
Source: {source_name}

Extract these fields (use null if not found):
- company_name: The primary company this signal is about
- funding_amount: Dollar amount if funding mentioned (e.g., "$50M", "$100 million")
- funding_round: Series A, B, C, Seed, etc.
- role_title: Job title if hiring signal
- key_people: Names of executives/founders mentioned
- industry: Company's industry/sector
- location: Geographic location if mentioned

Return ONLY valid JSON, no markdown formatting."""


def extract_entities(title: str, summary: str, source_name: str) -> dict:
    """
    Use GPT-4o to extract structured entities from signal content.
    Returns dict with extracted fields, empty dict on error.
    """
    if not settings.ai_enabled or not settings.openai_api_key:
        log.debug("ai_disabled_or_no_key", skipping="entity_extraction")
        return {}

    try:
        client = get_openai_client()

        prompt = EXTRACTION_PROMPT.format(
            title=title,
            summary=summary,
            source_name=source_name
        )

        response = client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {"role": "system", "content": "You are an entity extraction system. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=500,
            response_format={"type": "json_object"}
        )

        content = response.choices[0].message.content
        entities = json.loads(content)

        log.info("entities_extracted", company=entities.get("company_name"), fields=len([v for v in entities.values() if v]))
        return entities

    except json.JSONDecodeError as e:
        log.warning("entity_extraction_json_error", error=str(e))
        return {}
    except Exception as e:
        log.error("entity_extraction_failed", error=str(e))
        return {}
```
  </action>
  <verify>
    cd worker && python -c "from src.ai.extract import extract_entities; print('Entity extraction module OK')"
    grep "openai" worker/pyproject.toml  # Should find openai dependency
  </verify>
  <done>
    - OpenAI client singleton
    - Entity extraction with structured prompt
    - JSON response format for reliable parsing
    - Graceful degradation when AI disabled
  </done>
</task>

<task type="auto">
  <name>Task 2: Create classification and priority scoring module</name>
  <files>
    worker/src/ai/classify.py
  </files>
  <action>
**src/ai/classify.py** - Signal classification and scoring:
```python
from openai import OpenAI
from ..config import settings
from .extract import get_openai_client
import structlog
import json
from typing import Literal

log = structlog.get_logger()

SignalType = Literal['hiring', 'funding', 'expansion', 'partnership', 'product_launch', 'leadership_change']
Priority = Literal['high', 'medium', 'low']


CLASSIFICATION_PROMPT = """Classify this business signal and assess its sales priority.

Signal:
Title: {title}
Summary: {summary}
Source: {source_name}
Extracted Entities: {entities}

Classification rules:
- hiring: Company is recruiting, especially sales/growth roles
- funding: Company raised investment, new funding round
- expansion: Company entering new markets, opening offices
- partnership: Company forming alliances, integrations
- product_launch: Company releasing new products/features
- leadership_change: New executives, C-suite changes

Priority rules (for sales outreach potential):
- high: Large funding ($50M+), C-suite hires, major expansions, enterprise-focused signals
- medium: Standard funding rounds, director-level hires, product launches
- low: Seed rounds, junior hires, minor updates

Return JSON with exactly these fields:
{{"signal_type": "one of the types above", "priority": "high|medium|low", "confidence": 0.0-1.0, "reasoning": "brief explanation"}}"""


def classify_signal(
    title: str,
    summary: str,
    source_name: str,
    entities: dict,
    fallback_type: str = "product_launch",
    fallback_priority: str = "medium"
) -> tuple[SignalType, Priority, float]:
    """
    Use GPT-4o to classify signal type and priority.

    Returns: (signal_type, priority, confidence)
    Falls back to provided defaults if AI fails.
    """
    if not settings.ai_enabled or not settings.openai_api_key:
        log.debug("ai_disabled_or_no_key", skipping="classification")
        return fallback_type, fallback_priority, 0.0

    try:
        client = get_openai_client()

        prompt = CLASSIFICATION_PROMPT.format(
            title=title,
            summary=summary,
            source_name=source_name,
            entities=json.dumps(entities) if entities else "None"
        )

        response = client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {"role": "system", "content": "You are a signal classification system. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=200,
            response_format={"type": "json_object"}
        )

        content = response.choices[0].message.content
        result = json.loads(content)

        signal_type = result.get("signal_type", fallback_type)
        priority = result.get("priority", fallback_priority)
        confidence = result.get("confidence", 0.5)

        # Validate signal_type
        valid_types = ['hiring', 'funding', 'expansion', 'partnership', 'product_launch', 'leadership_change']
        if signal_type not in valid_types:
            log.warning("invalid_signal_type", received=signal_type, using=fallback_type)
            signal_type = fallback_type

        # Validate priority
        valid_priorities = ['high', 'medium', 'low']
        if priority not in valid_priorities:
            log.warning("invalid_priority", received=priority, using=fallback_priority)
            priority = fallback_priority

        log.info("signal_classified", type=signal_type, priority=priority, confidence=confidence)
        return signal_type, priority, confidence

    except json.JSONDecodeError as e:
        log.warning("classification_json_error", error=str(e))
        return fallback_type, fallback_priority, 0.0
    except Exception as e:
        log.error("classification_failed", error=str(e))
        return fallback_type, fallback_priority, 0.0


def score_priority(
    signal_type: str,
    entities: dict,
    source_name: str
) -> Priority:
    """
    Rule-based priority scoring as fallback/supplement to AI.
    Used when AI confidence is low or AI is disabled.
    """
    score = 50  # Start at medium

    # Funding signals: check amount
    if signal_type == 'funding':
        amount = entities.get('funding_amount', '')
        if any(s in str(amount).lower() for s in ['billion', '100m', '50m']):
            score += 30
        elif any(s in str(amount).lower() for s in ['series c', 'series d', 'series e']):
            score += 20
        elif 'seed' in str(amount).lower() or 'pre-seed' in str(amount).lower():
            score -= 20

    # Hiring signals: check role seniority
    if signal_type == 'hiring':
        role = entities.get('role_title', '').lower()
        if any(s in role for s in ['vp', 'vice president', 'director', 'head of', 'chief', 'cxo']):
            score += 25
        elif any(s in role for s in ['manager', 'lead', 'senior']):
            score += 10

    # Leadership changes: C-suite is always high priority
    if signal_type == 'leadership_change':
        people = str(entities.get('key_people', '')).lower()
        if any(s in people for s in ['ceo', 'cto', 'cfo', 'coo', 'founder']):
            score += 30

    # Source reliability bonus
    trusted_sources = ['techcrunch', 'bloomberg', 'forbes', 'wall street journal']
    if any(s in source_name.lower() for s in trusted_sources):
        score += 10

    # Convert score to priority
    if score >= 70:
        return 'high'
    elif score >= 40:
        return 'medium'
    else:
        return 'low'
```
  </action>
  <verify>
    cd worker && python -c "from src.ai.classify import classify_signal, score_priority; print('Classification module OK')"
  </verify>
  <done>
    - AI-powered classification with confidence scores
    - Rule-based scoring as fallback
    - Validates AI output against allowed values
    - Graceful degradation when AI unavailable
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate AI pipeline into scraper flow</name>
  <files>
    worker/src/scrapers/base.py
    worker/src/main.py
    worker/.env.example
  </files>
  <action>
**Update src/scrapers/base.py** - Add AI enrichment method:
```python
from abc import ABC, abstractmethod
from ..models import Signal
from ..ai import extract_entities, classify_signal, score_priority
from ..config import settings
import structlog

log = structlog.get_logger()


class BaseScraper(ABC):
    name: str = "base"

    @abstractmethod
    async def scrape(self) -> list[Signal]:
        """Scrape source and return list of signals."""
        pass

    def log_result(self, signals: list[Signal]):
        log.info(
            "scrape_complete",
            scraper=self.name,
            signal_count=len(signals)
        )

    def enrich_signal(self, signal: Signal) -> Signal:
        """
        Enrich a signal with AI-extracted entities and classification.
        Updates signal in place and returns it.
        """
        if not settings.ai_enabled:
            return signal

        try:
            # Step 1: Extract entities
            entities = extract_entities(
                title=signal.title,
                summary=signal.summary,
                source_name=signal.source_name
            )

            # Merge extracted entities into metadata
            if entities:
                signal.metadata = {**signal.metadata, **entities}

            # Step 2: Classify signal type and priority
            ai_type, ai_priority, confidence = classify_signal(
                title=signal.title,
                summary=signal.summary,
                source_name=signal.source_name,
                entities=entities,
                fallback_type=signal.signal_type,
                fallback_priority=signal.priority
            )

            # Use AI classification if confident, otherwise keep rule-based
            if confidence >= 0.7:
                signal.signal_type = ai_type
                signal.priority = ai_priority
            else:
                # Low confidence: use rule-based scoring
                signal.priority = score_priority(
                    signal_type=signal.signal_type,
                    entities=entities,
                    source_name=signal.source_name
                )

            # Store confidence in metadata
            signal.metadata['ai_confidence'] = confidence
            signal.metadata['ai_enriched'] = True

            log.debug("signal_enriched", company=signal.company_name, type=signal.signal_type, priority=signal.priority)

        except Exception as e:
            log.warning("enrichment_failed", error=str(e), company=signal.company_name)
            signal.metadata['ai_enriched'] = False

        return signal
```

**Update src/main.py** - Apply enrichment in pipeline:
```python
import asyncio
import schedule
import time
import structlog
from .config import settings
from .scrapers.news import TechCrunchScraper
from .scrapers.jobs import JobBoardScraper
from .scrapers.company import CompanyWebsiteScraper
from .db.supabase import insert_signal

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
log = structlog.get_logger()

# Demo user ID - in production, signals would be assigned to users based on their preferences
DEMO_USER_ID = "00000000-0000-0000-0000-000000000000"


async def run_scrapers():
    """Run all scrapers, enrich with AI, and store results."""
    log.info("scrape_cycle_start", ai_enabled=settings.ai_enabled)

    scrapers = [
        TechCrunchScraper(),
        JobBoardScraper(),
        CompanyWebsiteScraper(),
    ]

    total_signals = 0
    enriched_signals = 0

    for scraper in scrapers:
        try:
            signals = await scraper.scrape()

            for signal in signals:
                # Enrich with AI before inserting
                enriched_signal = scraper.enrich_signal(signal)

                if enriched_signal.metadata.get('ai_enriched'):
                    enriched_signals += 1

                result = insert_signal(enriched_signal, DEMO_USER_ID)
                if result:
                    total_signals += 1

        except Exception as e:
            log.error("scraper_failed", scraper=scraper.name, error=str(e))

    log.info(
        "scrape_cycle_complete",
        total_signals=total_signals,
        ai_enriched=enriched_signals
    )


def job():
    """Wrapper to run async scrapers from sync scheduler."""
    asyncio.run(run_scrapers())


def main():
    log.info(
        "worker_starting",
        interval=settings.scrape_interval_minutes,
        ai_enabled=settings.ai_enabled,
        model=settings.openai_model
    )

    # Run immediately on start
    job()

    # Then schedule regular runs
    schedule.every(settings.scrape_interval_minutes).minutes.do(job)

    while True:
        schedule.run_pending()
        time.sleep(60)


if __name__ == "__main__":
    main()
```

**Update .env.example** - Add OpenAI vars:
```
# Supabase
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbG...

# Worker settings
SCRAPE_INTERVAL_MINUTES=30
LOG_LEVEL=INFO

# Bright Data proxy (optional - for job boards)
BRIGHT_DATA_USERNAME=brd-customer-xxx
BRIGHT_DATA_PASSWORD=xxx

# OpenAI (required for AI enrichment)
OPENAI_API_KEY=sk-xxx
OPENAI_MODEL=gpt-4o-mini
AI_ENABLED=true
```
  </action>
  <verify>
    cd worker && python -c "
from src.scrapers.base import BaseScraper
from src.main import run_scrapers
print('AI pipeline integrated into scraper flow')
"
    grep "enrich_signal" worker/src/main.py  # Should find enrichment call
    grep "ai_enabled" worker/src/main.py  # Should log AI status
  </verify>
  <done>
    - BaseScraper has enrich_signal method
    - Main loop applies AI enrichment before insert
    - Confidence threshold controls AI vs rule-based
    - Full pipeline: scrape -> dedupe -> enrich -> classify -> insert
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. AI modules exist and import:
   ```bash
   cd worker && python -c "
   from src.ai import extract_entities, classify_signal, score_priority
   print('AI modules OK')
   "
   ```

2. BaseScraper has enrichment:
   ```bash
   grep "def enrich_signal" worker/src/scrapers/base.py
   ```

3. Main applies AI before insert:
   ```bash
   grep -A2 "for signal in signals" worker/src/main.py | grep "enrich"
   ```

4. Config has all AI settings:
   ```bash
   grep -E "(openai|ai_enabled)" worker/src/config.py
   ```
</verification>

<success_criteria>
- OpenAI integration extracts entities from signals
- AI classification determines signal type with confidence
- Priority scoring uses AI + rule-based fallback
- Full pipeline: scrape -> dedupe -> AI enrich -> insert
- Graceful degradation when AI disabled or unavailable
</success_criteria>

<output>
After completion, create `.planning/phases/02-signal-ingestion/02-03-SUMMARY.md`

Include:
- AI pipeline architecture (extract -> classify -> score)
- Model choice (gpt-4o-mini for cost, gpt-4o for quality)
- Confidence threshold strategy
- Cost considerations and budget controls
- User setup for OpenAI API key
</output>
